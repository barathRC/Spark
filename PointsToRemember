
SPARK CLUSTER ABD RUNTIME ARCHITECTURE

Spark is a distributed cluster in-memory parallel processing framework
Spark application is also distributed (driver JVM app and executor JVM app)

Pyspark is a wrapper around Java wrapper inturn around spark core which is coded in Scala which runs on JVM
Pyspark driver main() connects with JVM application driver main() with Py4j
Scala directly JVM application driver

When you do a spark submit, the RM (YARN) will spin up an Application Master(AM) in one of the worker node (4core 16GB RAM out of 16core 64GB RAM worker node)
AM requests YARN RM to provision Executor containers. RM provisions the required Exec containers by spinning up them in the worker nodes 
and providing them to the App driver
App Driver starts executors in the exec containers

SparkSession is the Driver. Only one SparkSession is valid for an application similar to only oen driver.


SPARK SUBMIT

spark submit --class <main class guru.learnjourn.hellospark; not for pyspark> --master<yarn,local[3]> --deploy-mode<client or cluster> --conf<spark.executor.memoryOverhead=0.20>
--driver-cores <2> --driver-memory <8G> --num-executors <4> --executore-cores <4> --executor-memory<16G> <hello-spark.py/ hello-spark.jar>


DEPLOY MODES CLIENT AND CLUSTER

CLIENT
For dev test. Interactive shows results.
Driver is in the client machine. No AM. main() requests RM for executor resources.
Shen u log off log off from the client machine the driver is killed, execuors are orphans, application is killed.

CLUSTER
For production
Driver runs in worker node in the cluster.
Even if u log off from the client machine the application runs.


SPARK JOBS , STAGES, SHUFFLES, TASKS

DATAFRAME API - 
Transformations (Narrow dep [select(),filter(), drop(),withColumn()], Wide dep [join(),groupBy(),agg()])
Actions (Triggers jobs - read() write() take() collect(), count()

JOB
Every code block with an action is a job.

STAGE
Multiple narrow transf followed by a single wide transf results in a stage
Stage is sequential. Stage has multiple parallel tasks
Stage writes results to Write exhange. Next stage reads this from the read exchange.
read write echnage enables shuffle sort of data.

TASK
A task is the granular unit of work in each exec slot.
Task is eq to the num of i/p partitions. If 20 partitions , 20 tasks can run in parallel

If there are 20 tasks and 30 executor cores for a spark app, all the tasks can run in parallel
If there are 60 tasks, 30 tasks must wait for slot availability

Job is compiled in the driver to arrive a logical plan before spark sql engine optimizes it to a physical plan.
Similarly stages and tasks for each executor is determined by the driver itself!


SPARK SQL ENGINER AND QUERY PLAN

Spark API - SQL, Dataframe, Dataset 
Dataset not available for pyspark since dataset is statically typed.
Dataframe internally is a dataset.

spark job sends unresolved plan to Spark SQL engine
Each SQL expression is a job
              
                                   (Jobs Stages, tasks)               
ANALYSIS                          LOGICAL OPTIMIZATION      PHYSICAL PLANNING                                        CODE GENRENATION (codegen) 
Unres Logical Plan  - Logical plan  - Optimized Logical Plan  - pHYSICAL Plan - COST MODEL  - Selected Physical plan  - RDDs
                  CATALOG                                                       (Broadcast Join, Sort merge cost, Shuffle hash join costs)
                  (To check for correct col names, syntax, references, functions, commands)


Logging:-

Log4j - 
1) Create a Log4j config file (Logger, Configuration, Appender)

  log4j.root.category = WARN,console
  log4j.logger.guru.learning.jounrnal=INFO,console,file
  log4j.appender.file.File=$(spark.yarn.app.container.log.dir)/$(logfile.name).log
  
2) Config Spark JVM to pickup the Log4j config file.

  SPARK_HOME/conf/spark-defaults.conf
   spark.driver.extraJavaoptions  - Dlog4j.configuration=file:log4j.properties -Dspark.yarn.app.container.log.dir=app_logs Dlog.file.name=hello-spark
   
3) Create a python class to get Spark's Log4j instance which has logger , WARN, INFO, ERROR, DEBUG methods.

4) Invoke in the app's code.
  Logger = log4j(spark)
  logger.info(messages)
  

CONFIGURING SPARK SESSION:-
1) Env variables - SPARK_HOME HADOOP_HOME PYSPARK_HOME - cluster admin - precedence 4
2) SPARK_HOME/conf/spark-defaults.conf - cluster admin - precedence 3
3) spark-submit command line options - developer - precedence 2
4) sparkConf obj in the appl. - developer - precedence 1


PYTEST:-

from unittest import TestCase
def method()
self.assertEqual(df.count(),9,"Count is 9")


DATAFRAME SOURCES AND SINKS:-
DATAFRAME is a Row(DataSet)

json does not need inferSchema .
Parquet has schema inbuilt

SPARK DBS and TABLES:-

SQL Catalog

SPARK WAREHOUSE - spark/sql/warehouse/dir for managed tables
CATALOG METASTORE - HIVE METASTORE

1) Managed - 
2) External - specify location

.saveasTable()
(Both folders, files and tables can be accessed)
Invoke using spark.sql


SPARK MEMORY ALLOCATION:-

1) spark.driver.memory = 1 GB
                  +
   spark.driver.memoryOverhead (CONTAINER or NON-JVM process) = 0.1 (10% or 384 MB) (150 mb FOR CONTAINER PROCESS. REMAINING FOR PYSPARK)
                                                                bUFFER Exhange or reading parition data from remote store

spark.Executor.memory = 8GB 
(
Reserved Memory = 300MB SPARK ENGINE
8GB - 300 MB
Spark Memory = 4620 MB sparl.memory.fraction = 0.6
User Memory = 0.4 = 3080 mb
)
+
spark.Executor.memoryOverhead = 10% 800 MB (400 mb FOR CONTAINER PROCESS. REMAINING FOR PYSPARK)
bUT 8.8 gb depends purely on yarn.scheduler.maximum.allocation.mb. If it is 6 GB then u cant get 8.8 GB

spark.Executor.offHeap.Size = 0
spark.Executor.PySpark.memory = 0  



