
SPARK CLUSTER ABD RUNTIME ARCHITECTURE

Spark is a distributed cluster in-memory parallel processing framework
Spark application is also distributed (driver JVM app and executor JVM app)

Pyspark is a wrapper around Java wrapper inturn around spark core which is coded in Scala which runs on JVM
Pyspark driver main() connects with JVM application driver main() with Py4j
Scala directly JVM application driver

When you do a spark submit, the RM (YARN) will spin up an Application Master(AM) in one of the worker node (4core 16GB RAM out of 16core 64GB RAM worker node)
AM requests YARN RM to provision Executor containers. RM provisions the required Exec containers by spinning up them in the worker nodes 
and providing them to the App driver
App Driver starts executors in the exec containers


SPARK SUBMIT

spark submit --class <main class guru.learnjourn.hellospark; not for pyspark> --master<yarn,local[3]> --deploy-mode<client or cluster> --conf<spark.executor.memoryOverhead=0.20>
--driver-cores <2> --driver-memory <8G> --num-executors <4> --executore-cores <4> --executor-memory<16G> <hello-spark.py/ hello-spark.jar>


DEPLOY MODES CLIENT AND CLUSTER

CLIENT
For dev test. Interactive shows results.
Driver is in the client machine. No AM. main() requests RM for executor resources.
Shen u log off log off from the client machine the driver is killed, execuors are orphans, application is killed.

CLUSTER
For production
Driver runs in worker node in the cluster.
Even if u log off from the client machine the application runs.


SPARK JOBS , STAGES, SHUFFLES, TASKS

DATAFRAME API - 
Transformations (Narrow dep [select(),filter(), drop(),withColumn()], Wide dep [join(),groupBy(),agg()])
Actions (Triggers jobs - read() write() take() collect(), count()

JOB
Every code block with an action is a job.

STAGE
Multiple narrow transf followed by a single wide transf results in a stage
Stage is sequential. Stage has multiple parallel tasks
Stage writes results to Write exhange. Next stage reads this from the read exchange.
read write echnage enables shuffle sort of data.

TASK
A task is the granular unit of work in each exec slot.
Task is eq to the num of i/p partitions. If 20 partitions , 20 tasks can run in parallel

If there are 20 tasks and 30 executor cores for a spark app, all the tasks can run in parallel
If there are 60 tasks, 30 tasks must wait for slot availability

Job is compiled in the driver to arrive a logical plan before spark sql engine optimizes it to a physical plan.
Similarly stages and tasks for each executor is determined by the driver itself!


SPARK SQL ENGINER AND QUERY PLAN

Spark API - SQL, Dataframe, Dataset 
Dataset not available for pyspark since dataset is statically typed.
Dataframe internally is a dataset.

spark job sends unresolved plan to Spark SQL engine
Each SQL expression is a job
              
                                   (Jobs Stages, tasks)               
ANALYSIS                          LOGICAL OPTIMIZATION      PHYSICAL PLANNING                                        CODE GENRENATION (codegen) 
Unres Logical Plan  - Logical plan  - Optimized Logical Plan  - pHYSICAL Plan - COST MODEL  - Selected Physical plan  - RDDs
                  CATALOG                                                       (Broadcast Join, Sort merge cost, Shuffle hash join costs)
                  (To check for correct col names, syntax, references, functions, commands)







